<h1>Build ChatGPT &#8211; Frontend System Design Guide</h1>
<p>
  <img
    loading="lazy"
    decoding="async"
    class="alignnone size-full wp-image-1825"
    src="https://api.frontendlead.com/wp-content/uploads/2025/06/FrontendLead-48.png"
    alt="Design ChatGPT - Frontend System Design"
    width="1920"
    height="1080"
    srcset="
      https://api.frontendlead.com/wp-content/uploads/2025/06/FrontendLead-48.png          1920w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/FrontendLead-48-300x169.png   300w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/FrontendLead-48-1024x576.png 1024w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/FrontendLead-48-768x432.png   768w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/FrontendLead-48-1536x864.png 1536w
    "
    sizes="(max-width: 1920px) 100vw, 1920px"
  />
</p>
<p>
  We‚Äôll walk through the frontend system design for building a ChatGPT-style web
  application. Our goal is to design an interface that lets users enter prompts,
  stream responses word by word, maintains a conversation history, and
  optionally supports model switching (for example, toggling between GPT-3.5 and
  GPT-4). We‚Äôll focus on building a frontend that‚Äôs fast, scalable, and
  production-ready, capable of serving millions of users with smooth, real-time
  experiences. You‚Äôll see estimates for scale, functional requirements, and the
  key non-functional criteria that make for a robust, user-friendly AI chat app.
</p>
<hr />
<h2>Requirements</h2>
<p>
  <img
    loading="lazy"
    decoding="async"
    class="alignnone size-full wp-image-1815"
    src="https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.01.19-AM.png"
    alt="Build GPT Requirements"
    width="450"
    height="435"
    srcset="
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.01.19-AM.png         450w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.01.19-AM-300x290.png 300w
    "
    sizes="(max-width: 450px) 100vw, 450px"
  />
</p>
<ol>
  <li><strong>Prompt input:</strong> User enters a prompt.</li>
  <li>
    <strong>Streaming response:</strong> Chat response streams words word by
    word, similar to ChatGPT.
  </li>
  <li>
    <strong>Conversation history:</strong> Threads of past chats per user.
  </li>
  <li>
    <strong>Model switching (optional):</strong> Choose between GPT-3.5, GPT-4,
    etc.
  </li>
</ol>
<h2>Scalability Estimates</h2>
<table border="1" cellspacing="0" cellpadding="6">
  <thead>
    <tr>
      <th>Metric</th>
      <th>Estimate</th>
      <th>Calculation</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Daily Active Users (DAUs)</td>
      <td>10M</td>
      <td>10 million users per day</td>
      <td></td>
    </tr>
    <tr>
      <td>Conversations per User</td>
      <td>5</td>
      <td>10M x 5 = 50M conversations/day</td>
      <td></td>
    </tr>
    <tr>
      <td>Messages per Conversation</td>
      <td>4</td>
      <td>50M x 4 = 200M messages/day</td>
      <td></td>
    </tr>
    <tr>
      <td>Average Message Size</td>
      <td>100 bytes</td>
      <td>200M x 100 bytes = 20GB/day</td>
      <td>~7.3TB/year</td>
    </tr>
    <tr>
      <td>Database Requirements</td>
      <td>Scalable (NoSQL/Replica DBs)</td>
      <td>&#8211;</td>
      <td>Must handle read-heavy workloads, suggest using Redis for caching</td>
    </tr>
  </tbody>
</table>
<h2>Non-Functional Requirements</h2>
<ul>
  <li>Real-time streaming (low latency)</li>
  <li>High availability</li>
  <li>Scalable backend</li>
  <li>Secure data handling</li>
  <li>Mobile-responsive UI</li>
  <li>Rate limiting or usage limits</li>
  <li>Fallback for slow networks/retry</li>
  <li>Performance</li>
</ul>
<hr />
<h2>Frontend Architecture Overview</h2>
<p>
  <img
    loading="lazy"
    decoding="async"
    class="alignnone size-full wp-image-1816"
    src="https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.05.40-AM.png"
    alt="ChatGPT Frontend Architecture "
    width="1093"
    height="1056"
    srcset="
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.05.40-AM.png          1093w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.05.40-AM-300x290.png   300w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.05.40-AM-1024x989.png 1024w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.05.40-AM-768x742.png   768w
    "
    sizes="(max-width: 1093px) 100vw, 1093px"
  />
</p>
<p>
  Let‚Äôs break down how to architect a robust, scalable frontend for ChatGPT. The
  goal is to create a web app that feels instant, smooth, and reliable‚Äîeven
  under massive user load. We want users to feel like they‚Äôre chatting with an
  assistant in real-time, with instant feedback, seamless navigation, and a
  persistent conversation history. To deliver this, we combine modern React
  design patterns, real-time data streaming, and a focus on security and
  performance.
</p>
<p>
  At a high level, the system follows a modular approach. Each core
  responsibility‚Äîsuch as message input, chat history, and response
  streaming‚Äîlives in its own React component, making the app easier to maintain
  and extend. Data flow is predictable and managed using a central state store,
  usually implemented with React Context, Redux, or Zustand. All data fetching
  and mutation routes go through a dedicated Network Manager module, which keeps
  side effects outside the view layer. Let‚Äôs dive into each area in detail.
</p>
<h3>Component Structure and Responsibilities</h3>
<p>
  The frontend is made up of composable React components that each handle a
  specific part of the UI or workflow. Here‚Äôs a breakdown of the major
  components and their responsibilities:
</p>
<ul>
  <li>
    <strong>App:</strong> The root component. Handles top-level state,
    authentication, and initial data fetches. Renders child components for chat
    windows, navigation, and message input.
    <pre><code>
&lt;App&gt;
  &lt;ChatWindow /&gt;
  &lt;MessageList /&gt;
  &lt;TypingIndicator /&gt;
  &lt;MessageInput /&gt;
  &lt;ChatWindows /&gt;
&lt;/App&gt;
    </code></pre>
  </li>
  <li>
    <strong>ChatWindow:</strong> Displays the active conversation, including
    both user and assistant messages. Manages infinite scroll for older messages
    and updates as new responses stream in.
  </li>
  <li>
    <strong>MessageList:</strong> A virtualized list of all messages in the
    current chat. Designed for efficiency, it only renders what‚Äôs visible on
    screen, allowing for fast scrolling, even with thousands of messages.
  </li>
  <li>
    <strong>TypingIndicator:</strong> Animates as the assistant types out each
    response. Connects to the real-time streaming logic, allowing users to see
    responses appear word by word.
  </li>
  <li>
    <strong>MessageInput:</strong> Where users type their prompts. Handles
    keyboard shortcuts, input sanitization, and triggers message sends.
  </li>
  <li>
    <strong>SendMessageBtn:</strong> The action button to submit a new prompt.
    Integrates reCAPTCHA or similar anti-abuse checks to prevent bots or spam.
  </li>
  <li>
    <strong>ConvosContainer:</strong> Allows users to view all their past
    conversations. Includes features like infinite scroll, search, and quick
    switching between threads.
  </li>
</ul>
<h3>Network Layer and Data Management</h3>
<p>
  A Network Manager manages all API requests and streaming connections. This
  module abstracts away the details of HTTP requests and Server-Sent Events
  (SSE), so the rest of the app doesn‚Äôt have to worry about transport logic. It
  also centralizes error handling, retries, and analytics tracking, ensuring a
  consistent user experience.
</p>
<ul>
  <li>
    <strong>Network Manager:</strong> Handles:
    <ul>
      <li>Fetching conversation lists and message history</li>
      <li>Sending user prompts to the backend</li>
      <li>Receiving streamed responses from the server</li>
      <li>Managing auth headers and API tokens</li>
      <li>Tracking performance, errors, and user analytics</li>
    </ul>
  </li>
</ul>
<p>
  State management is handled through a central store, using either React
  Context, Redux, or Zustand. This global state holds conversation threads, user
  metadata, loading and error states, and UI preferences. It enables any
  component to react to updates and keeps the UI in sync with the backend.
</p>
<h3>Streaming Response Workflow</h3>
<p>
  A standout feature of ChatGPT is its real-time response streaming. Instead of
  waiting for a full reply, users see each word or token appear as soon as the
  backend model generates it. This is usually implemented with Server-Sent
  Events (SSE) or WebSockets.
</p>
<ol>
  <li>
    The user enters a prompt in <strong>MessageInput</strong> and clicks send.
  </li>
  <li><strong>Network Manager</strong> sends the prompt to the backend.</li>
  <li>
    The backend starts generating the response and pushes each token as it
    becomes available.
  </li>
  <li>
    The frontend listens to the SSE stream, updating¬†<strong
      >the MessageList</strong
    >
    in real-time¬†as each new word arrives.
  </li>
  <li>
    If the user switches threads or navigates away, the stream is paused or
    cancelled to save resources.
  </li>
</ol>
<p>
  This approach feels much faster to users and mirrors a real conversation. It
  also requires careful handling of partial messages, error states, and UI
  updates.
</p>
<h3>Conversation History and Virtualization</h3>
<p>
  Users expect their conversation history to be instantly available, even across
  thousands of chats. The frontend stores a list of recent threads, including
  metadata such as timestamps and model version. When a user opens a thread, the
  app fetches only a window of messages, loading older messages on demand with
  infinite scroll. This keeps the initial page load light and fast, even for
  power users.
</p>
<ul>
  <li>
    Recent chats are shown using a <strong>ConvosContainer</strong> component,
    often with search and filter options.
  </li>
  <li>
    Each chat is displayed in a¬†<strong>ChatWindow</strong>, utilizing¬†a
    virtualized <strong>MessageList</strong> to render large numbers of messages
    efficiently.
  </li>
  <li>
    Intersection observers detect when to load more messages or prefetch
    upcoming ones.
  </li>
</ul>
<hr />
<h2>State and Data Flow</h2>
<p>
  A predictable data flow is crucial for reliability and maintainability. Here‚Äôs
  a typical flow:
</p>
<ol>
  <li>
    User action triggers a state update (e.g., sending a prompt or switching
    threads).
  </li>
  <li>Network requests are fired through the Network Manager.</li>
  <li>Response streams are received and update the global store.</li>
  <li>UI components subscribe to store changes and update reactively.</li>
  <li>
    All errors and loading states are surfaced in the UI so the user always
    knows what‚Äôs happening.
  </li>
</ol>
<p>
  This separation of concerns makes debugging easier and ensures that all
  network or side-effect logic stays out of the UI components.
</p>
<h3>Performance, Prefetching, and Caching</h3>
<p>
  With high user volume and chatty interfaces, performance matters. Several
  strategies keep the app fast:
</p>
<ul>
  <li>
    <strong>Virtualized lists:</strong> Both message lists and chat lists only
    render what‚Äôs visible. This keeps scrolling smoothly, even with thousands of
    messages.
  </li>
  <li>
    <strong>Prefetching:</strong> When a user hovers over or opens a
    conversation, the app preloads data in the background to enhance
    performance. This minimizes perceived latency.
  </li>
  <li>
    <strong>Intelligent caching:</strong> Frequently accessed data, like recent
    threads and user profiles, is cached in-memory or in local storage.¬†Expiry
    policies and background refreshes ensure freshness without over-fetching.
  </li>
  <li>
    <strong>Bundle splitting and lazy loading:</strong> Load UI components and
    libraries only when needed. For example, advanced input editors or media
    handlers load on demand.
  </li>
  <li>
    <strong>Client-side error boundaries:</strong> Ensure that a crash in one
    component doesn‚Äôt take down the whole app. Surface-friendly error messages
    and allow users to recover.
  </li>
</ul>
<h3>Security and Input Sanitization</h3>
<p>
  Security is built in from the start. All user inputs are sanitized before
  rendering to prevent cross-site scripting (XSS) attacks. Because chat content
  might include HTML, code, or links, the frontend escapes all potentially
  dangerous characters. In React, this is typically handled automatically, but
  extra checks are used for any raw HTML.
</p>
<ul>
  <li>
    <strong>Rate limiting:</strong> Prevents bots from spamming the chat
    endpoint. The <strong>SendMessageBtn</strong> disables rapidly after each
    send and re-enables after a short delay.
  </li>
  <li>
    <strong>reCAPTCHA integration:</strong> Used before sending the request to
    the backend to ensure it is human.
  </li>
  <li>
    <strong>Content sanitization:</strong> Every user message is cleaned before
    it‚Äôs sent and before it‚Äôs rendered.
  </li>
</ul>
<h3>Mobile Responsiveness and Accessibility</h3>
<p>
  The frontend is designed to be mobile-first. All layouts are adaptable to
  different screen sizes, featuring touch-friendly controls and responsive type
  sizes. Keyboard navigation, ARIA labels, and screen reader support make the
  app accessible to everyone. Core functions, such as entering a prompt,
  navigating threads, and viewing message history, are fully operable with the
  keyboard alone.
</p>
<ul>
  <li>
    <strong>Accessible buttons and inputs:</strong> All interactive elements use
    proper roles and labels.
  </li>
  <li>
    <strong>Graceful error states:</strong> Users always get clear, actionable
    feedback if something goes wrong.
  </li>
  <li>
    <strong>Localizations:</strong> The app can support multiple languages and
    right-to-left layouts.
  </li>
</ul>
<h3>Extensible Input and Rich Content</h3>
<p>
  ChatGPT isn‚Äôt just plain text. Users may want to send images, code blocks, or
  rich-formatted messages. The frontend supports two types of editors:
</p>
<table border="1" cellspacing="0" cellpadding="6">
  <thead>
    <tr>
      <th>Component</th>
      <th>Purpose</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Extranea</td>
      <td>
        Handles plain text, supports emojis, auto-grows for multiline input, and
        is cross-browser compatible. Always sanitize input.
      </td>
      <td>Safe by default. Used for basic chat UI.</td>
    </tr>
    <tr>
      <td>ContentEditable</td>
      <td>
        Enables rich content: code blocks, images, WYSIWYG editing, and custom
        embeds.
      </td>
      <td>More complex, used for advanced or enterprise chat features.</td>
    </tr>
  </tbody>
</table>
<h2>Data Flow Summary</h2>
<p>
  The following steps summarize the complete frontend flow for a ChatGPT message
  exchange:
</p>
<ol>
  <li>
    The user opens the app and loads recent conversations. Only metadata is
    fetched initially.
  </li>
  <li>
    The user selects or starts a conversation. The relevant messages are fetched
    and displayed using a virtualized list.
  </li>
  <li>
    The user types a prompt and clicks send. The message is sanitized,
    rate-limited, and verified as human via reCAPTCHA.
  </li>
  <li>
    The prompt is sent to the backend using the Network Manager. The UI
    immediately shows a pending message with a typing indicator.
  </li>
  <li>
    The backend streams the response as tokens become available. The UI updates
    the chat window word by word, keeping the user engaged.
  </li>
  <li>
    If the connection fails, the frontend retries the request or shows an error
    with a retry button.
  </li>
  <li>
    Once complete, the message is saved to history, and the input field is reset
    for the following prompt.
  </li>
</ol>
<h3>Best Practices and Final Thoughts</h3>
<ul>
  <li>
    Keep components modular and responsibilities clear. Avoid tightly coupled
    logic.
  </li>
  <li>
    Always separate network effects from view logic using a dedicated manager or
    hooks.
  </li>
  <li>Prefer optimistic UI updates where possible for instant feedback.</li>
  <li>
    Utilize virtualization and prefetching to achieve scalable performance at
    any user scale.
  </li>
  <li>
    Invest early in accessibility and mobile-friendliness. It pays off as usage
    grows.
  </li>
  <li>
    Review all input and output handling for security and be proactive in
    sanitizing data.
  </li>
</ul>
<hr />
<h2>Data Model</h2>
<p>
  <img
    loading="lazy"
    decoding="async"
    class="alignnone size-full wp-image-1817"
    src="https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.15.08-AM.png"
    alt="ChatGPT Frontend Data Model - Frontend System Design"
    width="408"
    height="390"
    srcset="
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.15.08-AM.png         408w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.15.08-AM-300x287.png 300w
    "
    sizes="(max-width: 408px) 100vw, 408px"
  />
</p>
<p>
  The data model needs to be simple, fast to query, and easy to extend. At
  minimum, we need to support conversation threads, messages, and basic user
  info. Messages must support streaming, threading, and system metadata.
</p>
<h3>Core Types</h3>
<pre><code>
// Preview of a conversation in the list
type ConversationPreview = {
  id: string;
  title: string;
  updated_at: string;
};

// Full conversation details
type FullConversation = ConversationPreview &amp; {
model: string; // e.g. "gpt-4"
temperature: number; // sampling parameter
notes?: string; // user notes (optional)
};

// Message object
type Message = {
id: string; // unique identifier
conversation_id: string; // foreign key to Conversation
user_id?: string; // present for users
ip_address?: string; // for anonymous usage
text: string; // message content
gpt_response: boolean; // true if generated by GPT
parent_message_id?: string; // for threaded replies
created_at: string; // ISO timestamp
rating?: number; // optional score (e.g. 1‚Äì5)
};</code><code></code></pre>

<hr />
<h2>API Endpoints</h2>
<ul>
  <li>
    <code>GET /conversations</code> ‚Äì Returns a list of
    <code>ConversationPreview</code> for the user.
  </li>
  <li>
    <code>GET /conversations/{id}/messages</code> ‚Äì Returns an array of
    <code>Message</code> for a conversation.
  </li>
  <li>
    <code>POST /conversations/{id}/messages</code> ‚Äì Adds a new message (user
    prompt).
  </li>
</ul>
<h3>Message Streaming Format</h3>
<p>
  The backend streams message data as each token is generated. The frontend
  consumes these events in real time.
</p>
<pre><code>
// Message event stream format
data: {
  "id": "...",
  "choices": [
    {
      "delta": {
        "content": "Hello "
      }
    }
  ]
}

data: {
"choices": [
{
"delta": {
"content": "world"
}
}
]
}

data: [DONE]
</code></pre>

<h3>Notes</h3>
<ul>
  <li>Each message has an <code>id</code> for easy updates and threading.</li>
  <li><code>gpt_response</code> flags AI-generated vs user messages.</li>
  <li>
    <code>parent_message_id</code> Supports reply threads and future group chat
    features.
  </li>
  <li>
    Streaming allows the UI to update as each chunk arrives.
    <code>[DONE]</code> Signals completion.
  </li>
</ul>
<hr />
<h2>API Design</h2>
<p>
  <img
    loading="lazy"
    decoding="async"
    class="alignnone size-full wp-image-1818"
    src="https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.18.11-AM.png"
    alt="Design ChatGPT - Frontend System Design API Design"
    width="576"
    height="709"
    srcset="
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.18.11-AM.png         576w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.18.11-AM-244x300.png 244w
    "
    sizes="(max-width: 576px) 100vw, 576px"
  />
</p>
<p>
  The API should support standard CRUD operations for conversations and
  messages, as well as a real-time streaming endpoint for responses. For the
  chat streaming experience, using <strong>Server-Sent Events (SSE)</strong> is
  preferred. SSE allows the server to push each token or chunk as soon as it is
  ready, providing users with immediate feedback and a smoother user interface.
  Every API should use explicit, predictable request and response shapes.
  Authentication is handled via an auth token in the header.
</p>
<h3>Endpoints</h3>
<table border="1" cellspacing="0" cellpadding="6">
  <thead>
    <tr>
      <th>Action</th>
      <th>Method &amp; Endpoint</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Create Conversation</td>
      <td>POST /v1/conversation</td>
      <td>
        Create a new chat thread. Returns <code>{ id }</code> for the new
        conversation.
      </td>
    </tr>
    <tr>
      <td>Delete Conversation</td>
      <td>DELETE /v1/conversation/{id}</td>
      <td>
        Remove a chat thread for a user. Returns <code>{ success }</code>.
      </td>
    </tr>
    <tr>
      <td>View Conversations</td>
      <td>GET /v1/conversations?cursor=&#8230;&amp;limit=20</td>
      <td>
        List recent conversations (paginated). Returns array of
        <code>ConversationPreview</code>.
      </td>
    </tr>
    <tr>
      <td>View Conversation with Messages</td>
      <td>GET /v1/conversation/{id}/messages?cursor=&#8230;&amp;limit=20</td>
      <td>
        Fetch a full conversation and its messages. Returns
        <code>{ conversation, messages, nextCursor }</code>.
      </td>
    </tr>
    <tr>
      <td>Send Message (Streaming)</td>
      <td>POST /v1/conversation/{id}/message</td>
      <td>
        Sends a user prompt and streams the response from the backend.<br />
        <strong>Headers:</strong>
        <code>authToken, Idempotency-Key, Accept: text/event-stream</code><br />
        <strong>Body:</strong> <code>{ text, timestamp }</code><br />
        <strong>Response:</strong> Each token/chunk streamed as an SSE event.<br />
        <strong>Example stream:</strong>
        <pre><code>
data: {"id":"...","choices":[{"delta":{"content":"Hello "}}]}
data: {"choices":[{"delta":{"content":"world"}}]}
data: [DONE]
        </code></pre>
      </td>
    </tr>
    <tr>
      <td>Rate Message</td>
      <td>POST /v1/conversation/{id}/message/rating</td>
      <td>
        Send feedback or a rating for a message. Body: <code>{ score }</code>.
        Returns <code>{ success }</code>.
      </td>
    </tr>
  </tbody>
</table>
<h3>Notes on Streaming with SSE</h3>
<ul>
  <li>
    For streaming chat responses, Server-Sent Events (SSE) is preferred over
    regular HTTP fetch. SSE allows the backend to send each chunk as soon as
    it&#8217;s ready, maintaining the strict order. The client receives and
    renders each piece immediately.
  </li>
  <li>
    All message events follow a simple, predictable JSON structure, allowing the
    frontend to update the chat view in real-time.
  </li>
  <li>
    Use an <code>Idempotency-Key</code> header on message POSTs to prevent
    accidental duplicate sends.
  </li>
  <li>
    When the backend is done streaming, it sends a
    <code>data: [DONE]</code> signal to completion.
  </li>
  <li>All endpoints require an <code>authToken</code> header for security.</li>
  <li>
    Pagination is handled via <code>cursor</code> ¬†<code>limit</code> Parameters
    for listing messages and conversations.
  </li>
</ul>
<h3>Example: Consuming SSE on the Frontend</h3>
<p>
  <img
    loading="lazy"
    decoding="async"
    class="alignnone size-full wp-image-1819"
    src="https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.18.27-AM.png"
    alt="ChatGPT Streaming Response - Frontend System Design"
    width="805"
    height="731"
    srcset="
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.18.27-AM.png         805w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.18.27-AM-300x272.png 300w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.18.27-AM-768x697.png 768w
    "
    sizes="(max-width: 805px) 100vw, 805px"
  />
</p>
<pre><code>
// Set up SSE connection
const eventSource = new EventSource('/api/conversation/stream');

eventSource.onmessage = (event) =&gt; {
const parsed = JSON.parse(event.data);
if (parsed.choices) {
updateLastMessage(parsed.choices[0].delta.content);
} else if (event.data === "[DONE]") {
eventSource.close();
}
};
</code></pre>

<p>
  SSE ensures each token is delivered in strict sequence over TCP. This keeps
  the chat experience smooth and in order. For browsers or environments that
  don&#8217;t support SSE, consider a fallback to <code>fetch</code> with
  <code>ReadableStream</code> for token-wise updates.
</p>
<hr />
<h2>Streaming Optimizations</h2>
<p>
  <img
    loading="lazy"
    decoding="async"
    class="alignnone size-full wp-image-1820"
    src="https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.21.43-AM.png"
    alt="Design ChatGPT - Frontend System Design Streaming Optimizations"
    width="678"
    height="541"
    srcset="
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.21.43-AM.png         678w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.21.43-AM-300x239.png 300w
    "
    sizes="(max-width: 678px) 100vw, 678px"
  />
</p>
<p>
  Smooth, real-time chat streaming is key for a great user experience. To keep
  the UI responsive and reduce unnecessary work, we need to optimize both the
  frontend and backend streaming pipelines.
</p>
<h3>Frontend Streaming Strategies</h3>
<ul>
  <li>
    <strong>Avoid React re-renders per token.</strong> Instead of updating React
    state every token, use a ref or <code>innerHTML</code> to append tokens
    directly.
  </li>
  <li>
    <strong>Token buffering.</strong> Buffer 10‚Äì20 tokens, then update the DOM
    in one batch. This reduces rendering overhead, keeping scrolling smooth.
  </li>
  <li>
    <strong>Animation frame scheduling.</strong> Use
    <code>requestIdleCallback</code> or <code>requestAnimationFrame</code> to
    schedule DOM updates for best performance.
  </li>
  <li>
    <strong>Pause streaming in background tabs.</strong> Use
    <code>document.visibilityState</code> to detect when the tab is hidden and
    pause updates.
  </li>
</ul>
<h4>Sample Implementation</h4>
<p>
  <img
    loading="lazy"
    decoding="async"
    class="alignnone size-full wp-image-1821"
    src="https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.21.40-AM.png"
    alt="ChatGPT - Frontend System Design UseEffect"
    width="413"
    height="474"
    srcset="
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.21.40-AM.png         413w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.21.40-AM-261x300.png 261w
    "
    sizes="(max-width: 413px) 100vw, 413px"
  />
</p>
<pre><code>
// In your chat container component
const [tokenBuffer, setTokenBuffer] = useState&lt;string[]&gt;([]);
const flushRef = useRef(false);

useEffect(() =&gt; {
let animationFrameId: number;
const flushTokens = () =&gt; {
if (flushRef.current &amp;&amp; tokenBuffer.length &gt; 0) {
const snapshot = [...tokenBuffer];
setTokenBuffer([]); // clear buffer
setRenderedMessage(prev =&gt; prev + snapshot.join(""));
flushRef.current = false;
}
animationFrameId = requestAnimationFrame(flushTokens);
};
animationFrameId = requestAnimationFrame(flushTokens);
return () =&gt; cancelAnimationFrame(animationFrameId);
}, [tokenBuffer]);

// In your SSE event listener:
eventSource.onmessage = (event) =&gt; {
const newToken = event.data;
flushRef.current = true;
setTokenBuffer(prev =&gt; [...prev, newToken]);
};</code><code>
</code></pre>

<hr />
<h3>Backend Streaming Optimization</h3>
<p>
  <img
    loading="lazy"
    decoding="async"
    class="alignnone size-full wp-image-1823"
    src="https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.24.42-AM.png"
    alt="Design ChatGPT - Frontend System Design Optimizations"
    width="700"
    height="471"
    srcset="
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.24.42-AM.png         700w,
      https://api.frontendlead.com/wp-content/uploads/2025/06/Screenshot-2025-06-13-at-9.24.42-AM-300x202.png 300w
    "
    sizes="(max-width: 700px) 100vw, 700px"
  />
</p>
<ul>
  <li>
    <strong>Chunked transfer encoding.</strong> For Express/Node.js, use
    <code>res.write()</code> and <code>res.flush()</code> to stream tokens to
    the client as soon as they‚Äôre available.
  </li>
  <li>
    <strong>Token batching.</strong> Group tokens together (for example, send
    every 50 milliseconds) to reduce I/O overhead.
  </li>
  <li>
    <strong>Backpressure control.</strong> Monitor the client‚Äôs buffer size. If
    the client is slow, pause token generation (for WebSocket) or skip writes.
  </li>
</ul>
<h3>Latency Minimization</h3>
<ul>
  <li>
    <strong>Edge functions.</strong> Use platforms like Vercel or Cloudflare
    Workers to reduce time-to-first-byte (TTFB).
  </li>
  <li>
    <strong>Stream-start optimizations.</strong> Start emitting tokens as soon
    as the first token is ready. If model latency is high, pre-fill the first
    token (using a fake-typing fallback).
  </li>
</ul>
<p>&nbsp;</p>
<hr />
<h2>Handling User Edge Cases</h2>
<table border="1" cellspacing="0" cellpadding="6">
  <thead>
    <tr>
      <th>Case</th>
      <th>Strategy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>‚ùå User sends empty or spam input</td>
      <td>Debounce and validate input. Reject if empty or too short.</td>
    </tr>
    <tr>
      <td>‚ùå User sends multiple chats fast</td>
      <td>
        Use <code>AbortController</code> to cancel prior requests before
        starting new ones.
      </td>
    </tr>
    <tr>
      <td>üîÑ User reloads tab</td>
      <td>
        Persist the session in localStorage or the backend for seamless
        resumption.
      </td>
    </tr>
    <tr>
      <td>üîÅ Retry/Regenerate</td>
      <td>
        Save prompt/metadata in a queue to allow retrying or regenerating failed
        messages.
      </td>
    </tr>
    <tr>
      <td>üì∂ Network loss</td>
      <td>
        Graceful SSE/WebSocket reconnection using <code>Last-Event-ID</code> or
        message ID to resume streaming without duplicates.
      </td>
    </tr>
    <tr>
      <td>‚è≥ Token expired</td>
      <td>
        Prompt user to re-authenticate and retry the last request automatically.
      </td>
    </tr>
    <tr>
      <td>üö´ Abuse/spam</td>
      <td>
        Add rate limits per IP address or user ID and block VPNs or proxies as
        needed.
      </td>
    </tr>
  </tbody>
</table>
<h3>Bonus UX Enhancements</h3>
<ul>
  <li>Partial Markdown rendering: Highlight code blocks as they stream in.</li>
  <li>
    Smart autoscroll: Only scroll to the bottom if the user is already there,
    avoid interrupting mid-scroll.
  </li>
  <li>
    Typing placeholder: Show ‚ÄúGPT is typing&#8230;‚Äù animation for better user
    feedback.
  </li>
</ul>
<h3>Pro-Level Optimizations</h3>
<table border="1" cellspacing="0" cellpadding="6">
  <thead>
    <tr>
      <th>Area</th>
      <th>Suggestion</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Streaming Smoothness</td>
      <td>
        Use <code>requestAnimationFrame</code> batching when appending many
        tokens to avoid layout thrashing.
      </td>
    </tr>
    <tr>
      <td>Typing Indicator</td>
      <td>
        Debounce display if the user types very fast. Hide onBlur or after a
        short timeout.
      </td>
    </tr>
    <tr>
      <td>Retry Logic</td>
      <td>
        On SSE disconnect, use the <code>Last-Event-ID</code> header to resume
        the stream without losing progress.
      </td>
    </tr>
    <tr>
      <td>Client Memory</td>
      <td>
        Clear out-of-view messages in the virtualized list. Optionally set a
        <code>maxRenderCount</code> for lower memory usage.
      </td>
    </tr>
    <tr>
      <td>Analytics Hook</td>
      <td>
        Add a hook on each token render or user interaction for tools like
        Mixpanel.
      </td>
    </tr>
    <tr>
      <td>Mobile</td>
      <td>
        Adjust the virtual list to prevent input field overlap when the mobile
        keyboard opens (a common issue).
      </td>
    </tr>
  </tbody>
</table>
<hr />
<h2>Conclusion</h2>
<p>
  Building a modern ChatGPT frontend is all about creating a fast, scalable, and
  user-friendly chat experience. We broke down the key requirements, from core
  architecture and data modeling to API design, streaming strategies, and user
  edge case handling. Each decision‚Äîfrom using SSE for real-time streams to
  virtualizing message lists and optimizing for mobile‚Äîdirectly impacts how
  users experience the app at scale.
</p>
<p>
  The best frontend systems anticipate real-world usage patterns. They
  gracefully handle network hiccups, edge cases, and high user volume. By
  focusing on efficient rendering, robust input validation, and smooth
  streaming, you ensure the UI feels responsive at all times. Pro-level details
  like animation batching, session persistence, and Markdown support add polish
  that users notice.
</p>
<p>
  With this system design, you‚Äôre set up to deliver a ChatGPT frontend that
  feels modern, reliable, and ready for millions of users. Focus on simplicity,
  clarity, and performance, and you‚Äôll build an experience that stands out in
  the AI chat landscape.
</p>
